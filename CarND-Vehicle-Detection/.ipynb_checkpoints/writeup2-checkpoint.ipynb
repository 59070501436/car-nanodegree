{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Lambda\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Convolution2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.generic_utils import Progbar\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "\n",
    "#np.random.seed(1337)\n",
    "K.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_generator(latent_size, img_shape=(32,32,3)):\n",
    "    # we will map a pair of (z, L), where z is a latent vector and L is a\n",
    "    # label drawn from P_c, to image space (..., img_shape[0], img_shape[1], img_shape[2])\n",
    "    cnn = Sequential()\n",
    "\n",
    "    cnn.add(Dense(1024, input_dim=latent_size, activation='relu'))\n",
    "    cnn.add(Dense(int(128 * img_shape[0]/4 * img_shape[1]/4), activation='relu'))\n",
    "    cnn.add(Reshape((int(img_shape[0]/4), int(img_shape[1]/4), 128)))\n",
    "\n",
    "    # upsample to (..., img_shape[0]/2, img_shape[1]/2)\n",
    "    cnn.add(UpSampling2D(size=(2, 2)))\n",
    "    cnn.add(Convolution2D(256, 5, 5, border_mode='same',\n",
    "                          activation='relu', init='glorot_normal'))\n",
    "\n",
    "    # upsample to (..., img_shape[0], img_shape[1])\n",
    "    cnn.add(UpSampling2D(size=(2, 2)))\n",
    "    cnn.add(Convolution2D(128, 5, 5, border_mode='same',\n",
    "                          activation='relu', init='glorot_normal'))\n",
    "\n",
    "    # take a channel axis reduction\n",
    "    cnn.add(Convolution2D(img_shape[2], 2, 2, border_mode='same',\n",
    "                          activation='tanh', init='glorot_normal'))\n",
    "    \n",
    "    #cnn.add(Lambda(lambda x: (x * 127.5) + 127.5))\n",
    "    model = cnn\n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_discriminator(img_shape=(32,32,3)):\n",
    "    # build a relatively standard conv net, with LeakyReLUs as suggested in\n",
    "    # the reference paper\n",
    "    cnn = Sequential()\n",
    "    \n",
    "    #cnn.add(Lambda(lambda x: (x - 127.5) / 127.5, input_shape=img_shape))\n",
    "    cnn.add(Convolution2D(32, 3, 3, border_mode='same', subsample=(2, 2), input_shape=img_shape))\n",
    "    cnn.add(LeakyReLU())\n",
    "    cnn.add(Dropout(0.3))\n",
    "\n",
    "    cnn.add(Convolution2D(64, 3, 3, border_mode='same', subsample=(1, 1)))\n",
    "    cnn.add(LeakyReLU())\n",
    "    cnn.add(Dropout(0.3))\n",
    "\n",
    "    cnn.add(Convolution2D(128, 3, 3, border_mode='same', subsample=(2, 2)))\n",
    "    cnn.add(LeakyReLU())\n",
    "    cnn.add(Dropout(0.3))\n",
    "\n",
    "    cnn.add(Convolution2D(256, 3, 3, border_mode='same', subsample=(1, 1)))\n",
    "    cnn.add(LeakyReLU())\n",
    "    cnn.add(Dropout(0.3))\n",
    "\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dense(1, activation='sigmoid', name='generation'))\n",
    "\n",
    "    model = cnn\n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# latent size taken from the paper\n",
    "LATENT_SIZE = 100\n",
    "IMAGE_SHAPE = (32,32,3)\n",
    "\n",
    "# Adam parameters suggested in https://arxiv.org/abs/1511.06434\n",
    "adam_lr = 0.0002\n",
    "adam_beta_1 = 0.5\n",
    "\n",
    "def build_model():\n",
    "\n",
    "    # build the discriminator\n",
    "    discriminator = build_discriminator(IMAGE_SHAPE)\n",
    "    discriminator.compile(\n",
    "        optimizer=Adam(lr=adam_lr, beta_1=adam_beta_1),\n",
    "        #loss=['binary_crossentropy', 'sparse_categorical_crossentropy']\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # build the generator\n",
    "    generator = build_generator(LATENT_SIZE, IMAGE_SHAPE)\n",
    "    generator.compile(optimizer=Adam(lr=adam_lr, beta_1=adam_beta_1),\n",
    "                      loss='binary_crossentropy')\n",
    "\n",
    "    latent = Input(shape=(LATENT_SIZE,))\n",
    "\n",
    "    # get a fake image\n",
    "    #fake = generator([latent, image_class])\n",
    "    fake = generator(latent)\n",
    "\n",
    "    # we only want to be able to train generation for the combined model\n",
    "    discriminator.trainable = False\n",
    "    #fake, aux = discriminator(fake)\n",
    "    fake = discriminator(fake)\n",
    "    #combined = Model(input=[latent, image_class], output=[fake, aux])\n",
    "    combined = Model(input=latent, output=fake)\n",
    "\n",
    "    combined.compile(\n",
    "        optimizer=Adam(lr=adam_lr, beta_1=adam_beta_1),\n",
    "        #loss=['binary_crossentropy', 'sparse_categorical_crossentropy']\n",
    "        loss='binary_crossentropy'\n",
    "    )\n",
    "    \n",
    "    return generator, discriminator, combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "import sklearn\n",
    "\n",
    "def file_pattern(vehicle, train=True):\n",
    "    \n",
    "    if not vehicle:\n",
    "        pattern = 'data/non-vehicles/*/*.png'\n",
    "    elif train:\n",
    "        pattern = 'data/vehicles/KITTI_extracted/*.png'\n",
    "    else:\n",
    "        pattern = 'data/vehicles/GTI*/*.png'\n",
    "        \n",
    "    return pattern\n",
    "\n",
    "def image_generator(vehicle=True, train=True, batch_size=32, grayscale=False):\n",
    "    \n",
    "    pattern = file_pattern(vehicle, train)\n",
    "        \n",
    "    images = []\n",
    "    files = glob.glob(pattern)\n",
    "    \n",
    "    while 1:\n",
    "        \n",
    "        np.random.shuffle(files)\n",
    "        \n",
    "        for idx, file in enumerate(files):\n",
    "\n",
    "            img = cv2.imread(file)\n",
    "            img = cv2.resize(img, (IMAGE_SHAPE[:2]))\n",
    "            \n",
    "            if grayscale:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                img = img.reshape(img.shape + (1,))\n",
    "            \n",
    "            #print(np.min(img), np.max(img))\n",
    "            img = (img - 127.5) / 127.5\n",
    "            \n",
    "            images.append(img)\n",
    "\n",
    "            if len(images) % batch_size == 0:\n",
    "                result = np.array(images)\n",
    "                images.clear()\n",
    "                yield result #, ([1,] if vehicle else [0,]) * batch_size\n",
    "\n",
    "def train_generator(batch_size=32):\n",
    "    vehicle_gen = image_generator(True, True, batch_size)\n",
    "    \n",
    "    for v in vehicle_gen:\n",
    "        yield v\n",
    "                \n",
    "def test_generator(batch_size=32):\n",
    "    vehicle_gen = image_generator(True, False, batch_size//2)\n",
    "    non_vehicle_gen = image_generator(False, False, batch_size//2)\n",
    "    \n",
    "    for v in vehicle_gen:\n",
    "        for nv in non_vehicle_gen:\n",
    "            X = np.concatenate((v, nv))\n",
    "            y = np.array([1] * (batch_size//2) + [0] * (batch_size//2))\n",
    "            yield sklearn.utils.shuffle(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "(32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1544cd978>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkqbEXTgdBwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1558c2f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "train_gen = train_generator(batch_size=4)\n",
    "imgs = next(train_gen)\n",
    "print(imgs[0].shape)\n",
    "plt.imshow((imgs[0] + 1) /2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 20\n",
      "61/62 [============================>.] - ETA: 2s\n",
      "Testing for epoch 1:\n",
      "component              | loss | acc            \n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 0.84\n",
      "generator (test)       | 0.65\n",
      "discriminator (train)  | 0.59 | 0.69           \n",
      "discriminator (test)   | 0.71 | 0.50           \n",
      "Epoch 2 of 20\n",
      "11/62 [====>.........................] - ETA: 116s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-a76938f7fe38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mepoch_gen_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrick\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marcio/anaconda/envs/tensorflow/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1265\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marcio/anaconda/envs/tensorflow/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1602\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 1603\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   1604\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marcio/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marcio/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marcio/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/marcio/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marcio/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import pickle\n",
    "from PIL import Image\n",
    "\n",
    "NB_EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_gen = train_generator(batch_size=BATCH_SIZE)\n",
    "test_gen = test_generator(batch_size=BATCH_SIZE)\n",
    "nb_train = 4000\n",
    "nb_test = 200\n",
    "\n",
    "nb_batches_epoch = int(nb_train / BATCH_SIZE)\n",
    "nb_batches_test = int(nb_test / BATCH_SIZE)\n",
    "\n",
    "train_history = defaultdict(list)\n",
    "test_history = defaultdict(list)\n",
    "\n",
    "generator, discriminator, combined = build_model()\n",
    "\n",
    "for epoch in range(NB_EPOCHS):\n",
    "    print('Epoch {} of {}'.format(epoch + 1, NB_EPOCHS))\n",
    "\n",
    "    #nb_batches = int(X_train.shape[0] / batch_size)\n",
    "    progress_bar = Progbar(target=nb_batches_epoch)\n",
    "\n",
    "    epoch_gen_loss = []\n",
    "    epoch_disc_loss = []\n",
    "\n",
    "    for index in range(nb_batches_epoch):\n",
    "        progress_bar.update(index)\n",
    "        # generate a new batch of noise\n",
    "        noise = np.random.uniform(-1, 1, (BATCH_SIZE, LATENT_SIZE))\n",
    "\n",
    "        image_batch = next(train_gen)\n",
    "\n",
    "        generated_images = generator.predict(noise, verbose=0)\n",
    "\n",
    "        #print(image_batch.shape, generated_images.shape)\n",
    "        X = np.concatenate((image_batch, generated_images))\n",
    "        y = np.array([1] * BATCH_SIZE + [0] * BATCH_SIZE)\n",
    "        \n",
    "        # see if the discriminator can figure itself out...\n",
    "        epoch_disc_loss.append(discriminator.train_on_batch(X, y))\n",
    "\n",
    "        # make new noise. we generate 2 * batch size here such that we have\n",
    "        # the generator optimize over an identical number of images as the\n",
    "        # discriminator\n",
    "        noise = np.random.uniform(-1, 1, (2 * BATCH_SIZE, LATENT_SIZE))\n",
    "        \n",
    "        # we want to train the generator to trick the discriminator\n",
    "        # For the generator, we want all the {fake, not-fake} labels to say\n",
    "        # not-fake\n",
    "        trick = np.ones(2 * BATCH_SIZE)\n",
    "        \n",
    "        discriminator.trainable = False\n",
    "        epoch_gen_loss.append(combined.train_on_batch(noise, trick))\n",
    "        discriminator.trainable = True\n",
    "\n",
    "    print('\\nTesting for epoch {}:'.format(epoch + 1))\n",
    "\n",
    "    discriminator_test_loss = discriminator.evaluate_generator(test_gen, nb_batches_test)\n",
    "    discriminator_train_loss = np.mean(np.array(epoch_disc_loss), axis=0)\n",
    "\n",
    "    # make new noise\n",
    "    noise = np.random.uniform(-1, 1, (2 * nb_test, LATENT_SIZE))\n",
    "    \n",
    "    trick = np.ones(2 * nb_test)\n",
    "\n",
    "    generator_test_loss = combined.evaluate(noise, trick, verbose=False)\n",
    "    generator_train_loss = np.mean(np.array(epoch_gen_loss), axis=0)\n",
    "\n",
    "    # generate an epoch report on performance\n",
    "    train_history['generator'].append(generator_train_loss)\n",
    "    train_history['discriminator'].append(discriminator_train_loss)\n",
    "\n",
    "    test_history['generator'].append(generator_test_loss)\n",
    "    test_history['discriminator'].append(discriminator_test_loss)\n",
    "\n",
    "    #print(*discriminator.metrics_names)\n",
    "    #print('{0:<22s} | {1:4s} | {2:15s} | {3:5s}'.format(\n",
    "    print('{0:<22s} | {1:4s} | {2:15s}'.format(\n",
    "        'component', *discriminator.metrics_names))\n",
    "    print('-' * 65)\n",
    "\n",
    "    #ROW_FMT = '{0:<22s} | {1:<4.2f} | {2:<15.2f} | {3:<5.2f}'\n",
    "    ROW_FMT = '{0:<22s} | {1:<4.2f} | {2:<15.2f}'\n",
    "    ROW_FMT_GEN = '{0:<22s} | {1:<4.2f}'\n",
    "    print(ROW_FMT_GEN.format('generator (train)',\n",
    "                         train_history['generator'][-1]))\n",
    "    print(ROW_FMT_GEN.format('generator (test)',\n",
    "                         test_history['generator'][-1]))\n",
    "    print(ROW_FMT.format('discriminator (train)',\n",
    "                         *train_history['discriminator'][-1]))\n",
    "    print(ROW_FMT.format('discriminator (test)',\n",
    "                         *test_history['discriminator'][-1]))\n",
    "\n",
    "    # save weights every epoch\n",
    "    generator.save_weights(\n",
    "        'save/params_generator_epoch_{0:03d}.hdf5'.format(epoch), True)\n",
    "    discriminator.save_weights(\n",
    "        'save/params_discriminator_epoch_{0:03d}.hdf5'.format(epoch), True)\n",
    "\n",
    "    # generate some digits to display\n",
    "    noise = np.random.uniform(-1, 1, (4, LATENT_SIZE))\n",
    "\n",
    "    # get a batch to display\n",
    "    generated_images = generator.predict(noise, verbose=0)\n",
    "    \n",
    "    #print(np.min(generated_images), np.max(generated_images))\n",
    "    # arrange them into a grid\n",
    "    img = (np.concatenate([r.reshape(-1, IMAGE_SHAPE[1], IMAGE_SHAPE[2])\n",
    "                               for r in np.split(generated_images, 2)\n",
    "                               ], axis=1) * 127.5 + 127.5)\n",
    "    #Image.fromarray(img).save(\n",
    "    cv2.imwrite(\n",
    "        'save/plot_epoch_{0:03d}_generated.png'.format(epoch), img)\n",
    "\n",
    "pickle.dump({'train': train_history, 'test': test_history},\n",
    "            open('acgan-history.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
